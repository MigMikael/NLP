{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NG4J1M-Jgnp"
   },
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EayaVVIWJ4Ta"
   },
   "source": [
    "### Create a Chunk Parser : \n",
    "An NP chunk should be formed whenever the chunker finds ***an optional determiner (DT)*** followed by ***any number of adjectives (JJ)*** and then ***a noun (NN)***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "14kytolwJSXM"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar = \"CHUNK_NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "# result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Xz1N3ngo6un"
   },
   "source": [
    "## Tag Patterns\n",
    "A tag pattern is a sequence of part-of-speech tags\n",
    "delimited using angle brackets, e.g. < DT >?< JJ >*< NN >."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yievuOSPp4I9"
   },
   "source": [
    "## Chunking with Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WM3R-U8Pp2aV"
   },
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "CHUNK_NP: {<DT|PP\\$>?<JJ>*<NN>} # chunk determiner/possessive, adjectives and noun\n",
    "{<NNP>+} # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
    "(\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hpElwuHJqsAW"
   },
   "source": [
    "### Chunk two consecutive nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "d1156E4Rqifd"
   },
   "outputs": [],
   "source": [
    "nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]\n",
    "grammar = \"CHUNK_NP: {<NN><NN>} \"            \n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7l0XM3TrZk1"
   },
   "source": [
    "**Update Grammar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LwUD1kOurYgc"
   },
   "outputs": [],
   "source": [
    "nouns = [(\"money\", \"NN\"), (\"market\", \"NN\"), (\"fund\", \"NN\")]   \n",
    "grammar = \"CHUNK_NP: {<NN>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5w1GxQ8ivQM6"
   },
   "source": [
    "## Expore Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RuJn_x9GyKfQ"
   },
   "source": [
    "\n",
    "\n",
    "*   **Tagged Sentence** = [('However', 'WRB'), (',', ','), ('the', 'AT'), ('jury', 'NN'), ('said', 'VBD'), ('it', 'PPS'), ('believes', 'VBZ'), ('``', '``'), ('these', 'DTS'), ('two', 'CD'), ('offices', 'NNS'), ('should', 'MD'), ('be', 'BE'), ('combined', 'VBN'), ('to', 'TO'), ('achieve', 'VB'), ('greater', 'JJR'), ('efficiency', 'NN'), ('and', 'CC'), ('reduce', 'VB'), ('the', 'AT'), ('cost', 'NN'), ('of', 'IN'), ('administration', 'NN'), (\"''\", \"''\"), ('.', '.')]\n",
    "\n",
    "*   cp.parse(sent) = tree = (S\n",
    "  However/WRB\n",
    "  ,/,\n",
    "  the/AT\n",
    "  jury/NN\n",
    "  said/VBD\n",
    "  it/PPS\n",
    "  believes/VBZ\n",
    "  ``/``\n",
    "  these/DTS\n",
    "  two/CD\n",
    "  offices/NNS\n",
    "  should/MD\n",
    "  be/BE\n",
    "  **(CHUNK combined/VBN to/TO achieve/VB)**\n",
    "  greater/JJR\n",
    "  efficiency/NN\n",
    "  and/CC\n",
    "  reduce/VB\n",
    "  the/AT\n",
    "  cost/NN\n",
    "  of/IN\n",
    "  administration/NN\n",
    "  ''/''\n",
    "  ./.)\n",
    "  \n",
    "\n",
    "*   if subtree.label() == 'CHUNK' : print(subtree) ==> (CHUNK serve/VB to/TO protect/VB)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WQ0yqNMjvUbC"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "cp = nltk.RegexpParser('CHUNK: {<V.*> <TO> <V.*>}')\n",
    "brown = nltk.corpus.brown\n",
    "for sent in brown.tagged_sents():       \n",
    "  tree = cp.parse(sent)\n",
    "  for subtree in tree.subtrees():\n",
    "    if subtree.label() == 'CHUNK':  \n",
    "      print(sent), print(tree),print(subtree)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IbkD6JTMjMlq"
   },
   "source": [
    "## Chinking \n",
    "Sometimes it is easier to define what we want to exclude from a chunk. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UahxKS2Qj7ca"
   },
   "source": [
    "Define a chink to be a sequence of tokens that is not included in a chunk such as barked/VBD at/IN is a chink:  }< VBD | IN >+{  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uU-alDlZjDBs"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "grammar = r\"\"\"\n",
    "            CHUNK_NP:\n",
    "            {<.*>+}      # Chunk everything\n",
    "            }<VBD|IN>+{  # Chink sequences of VBD and IN\n",
    "          \"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJCMRs_w11iS"
   },
   "source": [
    "## Representing Chunks: Tags vs Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLir945A2FMk"
   },
   "source": [
    "เราสามารถ represent Chunk โดย ใช้ IOB tags \n",
    "\n",
    "*   B - Begin\n",
    "*   O - Outside\n",
    "*   I - Inside\n",
    "\n",
    "ไปเรื่อยๆ ถ้าเริ่ม chunk ใหม่ เราก็เริ่ม B- แล้วก็ I ไป ถ้าไม่ใช่ Chunk ก็ O ไป \n",
    "\n",
    "> We PRP B-NP\n",
    "\n",
    "> saw VBD O\n",
    "\n",
    "> the DT B-NP\n",
    "\n",
    "> yellow JJ I-NP\n",
    "\n",
    "> dog NN I-NP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-Ta5x3W3dMz"
   },
   "source": [
    "# Developing and Evaluating Chunkers\n",
    "\n",
    "\n",
    "> **The CoNLL 2000 corpus** contains 270k words of Wall Street Journal\n",
    "text, divided into \"train\" and \"test\" portions, annotated with part-of-speech tags and chunk tags in the IOB format.\n",
    "\n",
    "\n",
    "\n",
    "> ** Read the 100th sentence of the \"train\" portion of the corpus:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-6uwY1F93_eD"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('conll2000')\n",
    "from nltk.corpus import conll2000\n",
    "print(conll2000.chunked_sents('train.txt')[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nGKkFlcH45t7"
   },
   "source": [
    "CoNLL 2000 corpus contains three chunk types: \n",
    "*   NP chunk =>  a couple \n",
    "*   VP chunk => to give\n",
    "*   PP chunk => because of\n",
    "\n",
    "> we can use the chunk_types argument to select them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nwtGFsy161w5"
   },
   "outputs": [],
   "source": [
    "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rriEm6Pu9TOA"
   },
   "source": [
    "## Simple Evaluation and Baselines\n",
    "\n",
    "### establishing a baseline\n",
    "\n",
    "ไม่ใส่กฎอะไรเลย ดูความถูกต้องทั้งหมด (IOB Accuracy) ดูว่าทาย NP ถูกเท่าไรด้วย Precision, Recall, F-Measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7INueGqf9iM_"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "cp = nltk.RegexpParser(\"\")\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1A6CjkY09syK"
   },
   "source": [
    "*The IOB tag accuracy indicates that more than a third of the words are tagged with O, i.e. not in an NP chunk. However, since our tagger did not find\n",
    "any chunks, its precision, recall, and f-measure are all zero.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-ROUlmG-d9f"
   },
   "source": [
    "### Naive tagger, look for all tags in NP chunk\n",
    "that looks for tags beginning with letters\n",
    "that are characteristic of noun phrase tags (e.g. CD, DT, and JJ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xd2xfUhj-5wm"
   },
   "outputs": [],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCuI9DWcANUd"
   },
   "source": [
    "### Noun Phrase Chunking with a Unigram Tagger\n",
    "\n",
    "> เราเอา Unigram Tagger มา predict   IOB Chunk tags โดยบอก POS ด้วย  \n",
    "> (UnigramTagger ดูว่าคำนั้นเกิดเป็น Type ไหนเยอะสุด ก็บอกเป็น Type นั้นเลย ไม่ดู Context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OGBJOhpKDRXz"
   },
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "  def __init__(self, train_sents):\n",
    "    train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                    for sent in train_sents]\n",
    "    self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "  def parse(self, sentence):\n",
    "    pos_tags = [pos for (word,pos) in sentence]\n",
    "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "    conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                  in zip(sentence, chunktags)]\n",
    "    return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkEwWD1SJh5H"
   },
   "source": [
    "sent :\n",
    "> < Tree, len() = 15>\n",
    "\n",
    "> _label:'S'\n",
    "\n",
    "> [0]:('At', 'IN')\n",
    "\n",
    ">[1]:Tree('NP', [('the', 'DT'), ('same', 'JJ'), ('time', 'NN')])\n",
    "\n",
    ">[2]:(',', ',')[3]:Tree('NP', [('he', 'PRP')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HarOlbF-K-jv"
   },
   "source": [
    "using tree2conlltags to map each chunk tree to a list of word,tag,chunk triples\n",
    "\n",
    "\n",
    "> nltk.chunk.tree2conlltags(sent) = \n",
    "[('At', 'IN', 'O'), ('the', 'DT', 'B-NP'), ('same', 'JJ', 'I-NP'), ('time', 'NN', 'I-NP'),\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "TOlHOTPIJjhf"
   },
   "outputs": [],
   "source": [
    "# use unigram tagger to find the IOB tag given its POS tag\n",
    "from nltk.corpus import conll2000\n",
    "test_sents = conll2000.chunked_sents(\"test.txt\", chunk_types=[\"NP\"])\n",
    "train_sents = conll2000.chunked_sents(\"train.txt\", chunk_types=[\"NP\"])\n",
    "unigram_chunker = UnigramChunker(train_sents)\n",
    "print( unigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "99QTeOAnEC7E"
   },
   "source": [
    "ดูว่า Model บอกอะไรเรา"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qLMBMuhtEGNr"
   },
   "outputs": [],
   "source": [
    "postags = sorted(set(pos for sent in train_sents\n",
    "                         for (word, pos) in sent.leaves()))\n",
    "print( unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kMk253oWJRh_"
   },
   "source": [
    "\n",
    "*   most punctuation marks occur outside of NP chunks except  \\# and \\$, which are used as currency markers.\n",
    "*   determiners (DT) and possessives (PRP \\$ and WP \\$) occur at the beginnings of NP chunks\n",
    "*   noun types (NN, NNP, NNPS, NNS) mostly occur inside of NP chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoJzlBGJMV0k"
   },
   "source": [
    "### bigram chunker\n",
    "\n",
    "* Change One Line     self.tagger = nltk.UnigramTagger(train_data) -> self.tagger = nltk.BigramTagger(train_data) \n",
    "* increase accuracy a bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qgI2SoxUM378"
   },
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "  def __init__(self, train_sents):\n",
    "    train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                    for sent in train_sents]\n",
    "    self.tagger = nltk.BigramTagger(train_data)  \n",
    "\n",
    "  def parse(self, sentence):\n",
    "    pos_tags = [pos for (word,pos) in sentence]\n",
    "    tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "    chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "    conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                  in zip(sentence, chunktags)]\n",
    "    return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4457,
     "status": "ok",
     "timestamp": 1525505460622,
     "user": {
      "displayName": "Kantinee Kat.",
      "photoUrl": "//lh3.googleusercontent.com/-cTx6FLLpB3o/AAAAAAAAAAI/AAAAAAAAHxE/6Q7cfvbHQJ0/s50-c-k-no/photo.jpg",
      "userId": "112728143482467190932"
     },
     "user_tz": -420
    },
    "id": "QLlPHZxjMhZc",
    "outputId": "2b408935-6213-47aa-e1e9-6df47b63f0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "# use bigram tagger to find the IOB tag given its POS tag\n",
    "from nltk.corpus import conll2000\n",
    "test_sents = conll2000.chunked_sents(\"test.txt\", chunk_types=[\"NP\"])\n",
    "train_sents = conll2000.chunked_sents(\"train.txt\", chunk_types=[\"NP\"])\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmYQxBboWTCe"
   },
   "source": [
    "All chunks are created entirely based on part-of-speech tags. How about these !!! Same POS but Diff Chunk\n",
    "\n",
    "\n",
    "*   Joey/NN sold/VBD the/DT farmer/NN rice/NN ./.                ======= [The farmer][rice]\n",
    "*   Nick/NN broke/VBD my/DT computer/NN monitor/NN ./.          ======== [my computer monitor]\n",
    "\n",
    "The content of the words is needed to maximize chunking performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZJJZaZjXcJP"
   },
   "source": [
    "### Training Classifier-Based Chunkers\n",
    "\n",
    "เอา word มาด้วย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "H6hZ532_YnpM"
   },
   "outputs": [],
   "source": [
    "def _npchunk_features(sentence, i, history):\n",
    "  features = {}\n",
    "  word, pos = sentence[i]\n",
    "  features[\"pos\"] = pos\n",
    "  # add previous POS tag\n",
    "  prevword, prevpos = \"<START>\", \"<START>\" if i == 0 else sentence[i-1]\n",
    "  features[\"prevpos\"] = prevpos\n",
    "  # add current word\n",
    "  features[\"word\"] = word\n",
    "  # more features\n",
    "  nextword, nextpos = \"<END>\", \"<END>\" if i == len(sentence) - 1 else sentence[i+1]\n",
    "  features[\"nextpos\"] = nextpos\n",
    "  features[\"prevpos+pos\"] = \"%s+%s\" % (prevpos, pos)\n",
    "  features[\"pos+nextpos\"] = \"%s+%s\" % (pos, nextpos)\n",
    "  # tags since last determiner\n",
    "  tags_since_dt = set()\n",
    "  for word, pos in sentence[:i]:\n",
    "    if pos == \"DT\":\n",
    "      tags_since_dt = set()\n",
    "    else:\n",
    "      tags_since_dt.add(pos)\n",
    "  features[\"tags_since_dt\"] = \"+\".join(sorted(tags_since_dt))\n",
    "  return features\n",
    "\n",
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "  def __init__(self, train_sents):\n",
    "    train_set = []\n",
    "    for tagged_sent in train_sents:\n",
    "      untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "      history = []\n",
    "      for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featureset = _npchunk_features(untagged_sent, i, history)\n",
    "        train_set.append((featureset, tag))\n",
    "        history.append(tag)\n",
    "    self.classifier = nltk.MaxentClassifier.train(train_set,\n",
    "      algorithm=\"GIS\", trace=0)\n",
    "\n",
    "  def tag(self, sentence):\n",
    "    history = []\n",
    "    for i, word in enumerate(sentence):\n",
    "      featureset = _npchunk_features(sentence, i, history)\n",
    "      tag = self.classifier.classify(featureset)\n",
    "      history.append(tag)\n",
    "    return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "  def __init__(self, train_sents):\n",
    "    tagged_sents = [[((w,t),c) for (w,t,c) in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "    self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "  def parse(self, sentence):\n",
    "    tagged_sents = self.tagger.tag(sentence)\n",
    "    conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "    return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 328287,
     "status": "ok",
     "timestamp": 1525508898536,
     "user": {
      "displayName": "Kantinee Kat.",
      "photoUrl": "//lh3.googleusercontent.com/-cTx6FLLpB3o/AAAAAAAAAAI/AAAAAAAAHxE/6Q7cfvbHQJ0/s50-c-k-no/photo.jpg",
      "userId": "112728143482467190932"
     },
     "user_tz": -420
    },
    "id": "C5EyQaSsYy4t",
    "outputId": "213da650-4193-4156-81aa-65da4c750960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Training stopped: keyboard interrupt\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  95.9%%\n",
      "    Precision:     88.0%%\n",
      "    Recall:        91.7%%\n",
      "    F-Measure:     89.8%%\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "test_sents = conll2000.chunked_sents(\"test.txt\", chunk_types=[\"NP\"])\n",
    "train_sents = conll2000.chunked_sents(\"train.txt\", chunk_types=[\"NP\"])\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print( chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBGUaFSib_rq"
   },
   "source": [
    "# Recursion in Linguistic Structure\n",
    "\n",
    "A multi-stage chunk grammar containing recursive rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 900,
     "status": "ok",
     "timestamp": 1525510061387,
     "user": {
      "displayName": "Kantinee Kat.",
      "photoUrl": "//lh3.googleusercontent.com/-cTx6FLLpB3o/AAAAAAAAAAI/AAAAAAAAHxE/6Q7cfvbHQJ0/s50-c-k-no/photo.jpg",
      "userId": "112728143482467190932"
     },
     "user_tz": -420
    },
    "id": "Wqi0W3jbb-vW",
    "outputId": "440bb59c-ca10-450a-a7fd-a56cd389c2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (CLAUSE\n",
      "    (NP Mary/NN)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "    NP : {<DT|JJ|NN.*>+}    # chunk sentences of DT,JJ,NN\n",
    "    PP : {<IN><NP>}         # chunk preposition followed by NP\n",
    "    VP : {<VB.*><NP|PP|CLAUSE>+$}  # chunk verb and their argument\n",
    "    CLAUSE : {<NP><VP>}     # chunk NP,VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar , loop =2) # parses sentence multiple times\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print( cp.parse(sentence))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 922,
     "status": "ok",
     "timestamp": 1525509866794,
     "user": {
      "displayName": "Kantinee Kat.",
      "photoUrl": "//lh3.googleusercontent.com/-cTx6FLLpB3o/AAAAAAAAAAI/AAAAAAAAHxE/6Q7cfvbHQJ0/s50-c-k-no/photo.jpg",
      "userId": "112728143482467190932"
     },
     "user_tz": -420
    },
    "id": "CbP7ePk1d6NQ",
    "outputId": "c02a0489-a1d5-49ec-f41d-f3366b250159"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP John/NNP)\n",
      "  thinks/VBZ\n",
      "  (CLAUSE\n",
      "    (NP Mary/NNP)\n",
      "    (VP\n",
      "      saw/VBD\n",
      "      (CLAUSE\n",
      "        (NP the/DT cat/NN)\n",
      "        (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))))\n"
     ]
    }
   ],
   "source": [
    "sentence2 = [(\"John\", \"NNP\"), (\"thinks\", \"VBZ\"), (\"Mary\", \"NNP\"),\n",
    "    (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"), (\"sit\", \"VB\"),\n",
    "    (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "print( cp.parse(sentence2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eilWk9x8fGGE"
   },
   "source": [
    "loop to specify the number of times the set of patterns should be run ครั้งเดียว VP หายตรง saw, think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TitbJYsugJfr"
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n0LIlZXQiN3n"
   },
   "source": [
    "NLTK provides a classifier that has already been trained to recognize named entities, accessed with the function nltk.ne_chunk(). If we set the\n",
    "parameter binary=True , then named entities are just tagged as NE; otherwise, the classifier adds category labels such as PERSON,\n",
    "ORGANIZATION, and LOCATION/GPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2449,
     "status": "ok",
     "timestamp": 1525511109002,
     "user": {
      "displayName": "Kantinee Kat.",
      "photoUrl": "//lh3.googleusercontent.com/-cTx6FLLpB3o/AAAAAAAAAAI/AAAAAAAAHxE/6Q7cfvbHQJ0/s50-c-k-no/photo.jpg",
      "userId": "112728143482467190932"
     },
     "user_tz": -420
    },
    "id": "TigHL8huiNWx",
    "outputId": "595acd5f-c46a-42b6-8a9a-c46c3ed6218d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /content/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /content/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /content/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n",
      "(S\n",
      "  The/DT\n",
      "  (NE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (NE Brooke/NNP)\n",
      "  T./NNP\n",
      "  Mossman/NNP\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (NE University/NNP)\n",
      "  of/IN\n",
      "  (NE Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (NE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(nltk.ne_chunk(sent, binary=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1022,
     "status": "ok",
     "timestamp": 1525511135516,
     "user": {
      "displayName": "Kantinee Kat.",
      "photoUrl": "//lh3.googleusercontent.com/-cTx6FLLpB3o/AAAAAAAAAAI/AAAAAAAAHxE/6Q7cfvbHQJ0/s50-c-k-no/photo.jpg",
      "userId": "112728143482467190932"
     },
     "user_tz": -420
    },
    "id": "Dz5x6scyjIJB",
    "outputId": "aabb3762-be27-4119-8701-d255544598ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (GPE U.S./NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  few/JJ\n",
      "  industrialized/VBN\n",
      "  nations/NNS\n",
      "  that/WDT\n",
      "  *T*-7/-NONE-\n",
      "  does/VBZ\n",
      "  n't/RB\n",
      "  have/VB\n",
      "  a/DT\n",
      "  higher/JJR\n",
      "  standard/NN\n",
      "  of/IN\n",
      "  regulation/NN\n",
      "  for/IN\n",
      "  the/DT\n",
      "  smooth/JJ\n",
      "  ,/,\n",
      "  needle-like/JJ\n",
      "  fibers/NNS\n",
      "  such/JJ\n",
      "  as/IN\n",
      "  crocidolite/NN\n",
      "  that/WDT\n",
      "  *T*-1/-NONE-\n",
      "  are/VBP\n",
      "  classified/VBN\n",
      "  *-5/-NONE-\n",
      "  as/IN\n",
      "  amphobiles/NNS\n",
      "  ,/,\n",
      "  according/VBG\n",
      "  to/TO\n",
      "  (PERSON Brooke/NNP T./NNP Mossman/NNP)\n",
      "  ,/,\n",
      "  a/DT\n",
      "  professor/NN\n",
      "  of/IN\n",
      "  pathlogy/NN\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION University/NNP)\n",
      "  of/IN\n",
      "  (PERSON Vermont/NNP College/NNP)\n",
      "  of/IN\n",
      "  (GPE Medicine/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.corpus.treebank.tagged_sents()[22]\n",
    "print(nltk.ne_chunk(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PDC86FtUkSyl"
   },
   "source": [
    "# Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EkvcF6unoL5"
   },
   "source": [
    "Once NE have been identified in a text, we then want to extract the relations that exist between them.\n",
    "\n",
    "\n",
    "* \\bword\\b - word boundary\n",
    "* Negative Lookahead (?!     )\n",
    "* .+ matches any character \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 946,
     "status": "ok",
     "timestamp": 1525512515609,
     "user": {
      "displayName": "Kantinee Kat.",
      "photoUrl": "//lh3.googleusercontent.com/-cTx6FLLpB3o/AAAAAAAAAAI/AAAAAAAAHxE/6Q7cfvbHQJ0/s50-c-k-no/photo.jpg",
      "userId": "112728143482467190932"
     },
     "user_tz": -420
    },
    "id": "scaQe_5Vnmv_",
    "outputId": "4a8e92c5-1377-485f-e56b-3561131a8a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package ieer to /content/nltk_data...\n",
      "[nltk_data]   Package ieer is already up-to-date!\n",
      "[ORG: 'WHYY'] 'in' [LOC: 'Philadelphia']\n",
      "[ORG: 'McGlashan &AMP; Sarrail'] 'firm in' [LOC: 'San Mateo']\n",
      "[ORG: 'Freedom Forum'] 'in' [LOC: 'Arlington']\n",
      "[ORG: 'Brookings Institution'] ', the research group in' [LOC: 'Washington']\n",
      "[ORG: 'Idealab'] ', a self-described business incubator based in' [LOC: 'Los Angeles']\n",
      "[ORG: 'Open Text'] ', based in' [LOC: 'Waterloo']\n",
      "[ORG: 'WGBH'] 'in' [LOC: 'Boston']\n",
      "[ORG: 'Bastille Opera'] 'in' [LOC: 'Paris']\n",
      "[ORG: 'Omnicom'] 'in' [LOC: 'New York']\n",
      "[ORG: 'DDB Needham'] 'in' [LOC: 'New York']\n",
      "[ORG: 'Kaplan Thaler Group'] 'in' [LOC: 'New York']\n",
      "[ORG: 'BBDO South'] 'in' [LOC: 'Atlanta']\n",
      "[ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "nltk.download('ieer')\n",
    "# IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
    "IN = re.compile(r'.*\\bin\\b')\n",
    "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
    "  for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
    "    print(nltk.sem.rtuple(rel))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "IbkD6JTMjMlq"
   ],
   "default_view": {},
   "name": "Workshop7_1.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
