{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import แพคเกจที่จำเป็น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "wordlemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## โหลด Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open('/Users/migmikael/Downloads/SMSSpamCollection.tsv', \"r\",encoding='utf-8', errors='ignore') as data_file:\n",
    "    for line in data_file:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        data.append((text, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5570"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\n\",\n",
       "  'ham'),\n",
       " (\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\",\n",
       "  'spam'),\n",
       " (\"Nah I don't think he goes to usf, he lives around here though\\n\", 'ham'),\n",
       " ('Even my brother is not like to speak with me. They treat me like aids patent.\\n',\n",
       "  'ham'),\n",
       " ('I HAVE A DATE ON SUNDAY WITH WILL!!\\n', 'ham')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## แกะ Features\n",
    "---\n",
    "###  วิธีที่ 1\n",
    "- แบ่งประโยคที่รับมาด้วย Tokenizer แปลงเป็นพิมพ์เล็กด้วย .lower และหารากศัพย์ด้วย wordlemmatizer\n",
    "- สร้าง feature dictionary ด้วย wordtokens ที่ได้จากขั้นตอนก่อนหน้า โดยตัดเอา stopword ออก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')\n",
    "def feature_extractor(sent):\n",
    "    features = {}\n",
    "    wordtokens = [wordlemmatizer.lemmatize(word.lower()) for word in word_tokenize(sent)]\n",
    "    \n",
    "    for word in wordtokens:\n",
    "        if word not in stopword:\n",
    "            features[word] = True\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': True, 'iron': True, 'man': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(\"I am the iron man.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### วิธีที่ 2\n",
    "- จากการสังเกตุพบว่าข้อความที่เป็นสแปมมักมีตัว ! $ ประกอบอยู่ด้วย\n",
    "- นอกจากนี้ข้อความที่เป็นสแปมมักมีอัตราส่วนตัวอักษรพิมพ์ใหญ่ต่อตัวอักษรทั้งหมดมากเป็นพิเศษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor2(text):\n",
    "    features = {}\n",
    "    if \"!\" in text:\n",
    "        features[\"!\"] = True\n",
    "    if \"$\" in text:\n",
    "        features[\"$\"] = True\n",
    "        \n",
    "    lowercase = list('abcdefghijklmnopqrstuvwxyz')\n",
    "    uppercase = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "    \n",
    "    num_lowercase = 0\n",
    "    num_uppercase = 0\n",
    "    \n",
    "    for char in text:\n",
    "        if char in lowercase:\n",
    "            num_lowercase += 1\n",
    "        elif char in uppercase:\n",
    "            num_uppercase += 1\n",
    "    if num_lowercase + num_uppercase == 0:\n",
    "        features[\"upper_ratio\"] = 0\n",
    "    else:\n",
    "        features[\"upper_ratio\"] = num_uppercase / (num_lowercase + num_uppercase)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': True, '$': True, 'upper_ratio': 0.2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor2(\"Buy Iron Man Figure just 40$ !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### วิธีที่ 3 \n",
    "- วิธีนี้ใช้การนับความถี่ของการปรากฏของคำต่างๆ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return [wordlemmatizer.lemmatize(word.lower()) for word in tokens]\n",
    "\n",
    "def feature_extractor3(text):\n",
    "    return {word: count for word, count in Counter(preprocess(text)).items() if not word in stopword}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 3, '$': 1, '40': 1, 'buy': 1, 'figure': 1, 'iron': 1, 'man': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor3(\"Buy Iron Man Figure just 40$ !!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## สร้าง Featureset จากวิธีสามวิธีด้านบน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({\"'ve\": True,\n",
       "   '.': True,\n",
       "   'blessing': True,\n",
       "   'breather': True,\n",
       "   'fulfil': True,\n",
       "   'granted': True,\n",
       "   'help': True,\n",
       "   'promise': True,\n",
       "   'right': True,\n",
       "   'searching': True,\n",
       "   'take': True,\n",
       "   'thank': True,\n",
       "   'time': True,\n",
       "   'wonderful': True,\n",
       "   'wont': True,\n",
       "   'word': True},\n",
       "  'ham')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets1 = [(feature_extractor(sms), label) for (sms, label) in data]\n",
    "featuresets1[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'upper_ratio': 0.019230769230769232}, 'ham')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets2 = [(feature_extractor2(sms), label) for (sms, label) in data]\n",
    "featuresets2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({\"'ve\": 1,\n",
       "   '.': 3,\n",
       "   'blessing': 1,\n",
       "   'breather': 1,\n",
       "   'fulfil': 1,\n",
       "   'granted': 1,\n",
       "   'help': 1,\n",
       "   'promise': 2,\n",
       "   'right': 1,\n",
       "   'searching': 1,\n",
       "   'take': 1,\n",
       "   'thank': 1,\n",
       "   'time': 1,\n",
       "   'wonderful': 1,\n",
       "   'wont': 1,\n",
       "   'word': 1},\n",
       "  'ham')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets3 = [(feature_extractor3(sms), label) for (sms, label) in data]\n",
    "featuresets3[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## แบ่งข้อมูลออกเป็น Train และ Test Set\n",
    "- ใช้อัตราส่วน Train : Test ที่ 80 : 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set1 size = 4456, test_set1 size = 1114\n"
     ]
    }
   ],
   "source": [
    "size = int(len(featuresets1) * 0.8)\n",
    "train_set1, test_set1 = featuresets1[:size], featuresets1[size:]\n",
    "print(\"train_set1 size = %d, test_set1 size = %d\" % (len(train_set1), len(test_set1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set2 size = 4456, test_set2 size = 1114\n"
     ]
    }
   ],
   "source": [
    "size = int(len(featuresets1) * 0.8)\n",
    "train_set2, test_set2 = featuresets2[:size], featuresets2[size:]\n",
    "print(\"train_set2 size = %d, test_set2 size = %d\" % (len(train_set2), len(test_set2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set3 size = 4456, test_set3 size = 1114\n"
     ]
    }
   ],
   "source": [
    "size = int(len(featuresets3) * 0.8)\n",
    "train_set3, test_set3 = featuresets3[:size], featuresets3[size:]\n",
    "print(\"train_set3 size = %d, test_set3 size = %d\" % (len(train_set3), len(test_set3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train ตัว Classify ด้วย NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8994614003590664\n",
      "Most Informative Features\n",
      "                 service = True             spam : ham    =    155.0 : 1.0\n",
      "                   nokia = True             spam : ham    =    103.8 : 1.0\n",
      "                     txt = True             spam : ham    =     93.8 : 1.0\n",
      "                      uk = True             spam : ham    =     91.8 : 1.0\n",
      "                  urgent = True             spam : ham    =     90.6 : 1.0\n",
      "                    code = True             spam : ham    =     87.5 : 1.0\n",
      "                      16 = True             spam : ham    =     87.5 : 1.0\n",
      "                      po = True             spam : ham    =     70.5 : 1.0\n",
      "                delivery = True             spam : ham    =     66.2 : 1.0\n",
      "                   award = True             spam : ham    =     66.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier1 = NaiveBayesClassifier.train(train_set1)\n",
    "print(\"Accuracy :\", classify.accuracy(classifier1, test_set1))\n",
    "classifier1.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9039497307001796\n",
      "Most Informative Features\n",
      "             upper_ratio = 0.041666666666666664    ham : spam   =     14.8 : 1.0\n",
      "             upper_ratio = 0.045454545454545456    ham : spam   =     14.0 : 1.0\n",
      "             upper_ratio = 0.058823529411764705    ham : spam   =     13.1 : 1.0\n",
      "             upper_ratio = 0.1568627450980392   spam : ham    =     12.6 : 1.0\n",
      "             upper_ratio = 0.04              ham : spam   =     11.6 : 1.0\n",
      "             upper_ratio = 0.034482758620689655    ham : spam   =     10.9 : 1.0\n",
      "             upper_ratio = 0.03571428571428571    ham : spam   =     10.7 : 1.0\n",
      "             upper_ratio = 0.038461538461538464    ham : spam   =     10.5 : 1.0\n",
      "             upper_ratio = 0.15             spam : ham    =      9.8 : 1.0\n",
      "             upper_ratio = 0.0784313725490196   spam : ham    =      9.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier2 = NaiveBayesClassifier.train(train_set2)\n",
    "print(\"Accuracy :\", classify.accuracy(classifier2, test_set2))\n",
    "classifier2.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8931777378815081\n",
      "Most Informative Features\n",
      "                 service = 1                spam : ham    =    152.3 : 1.0\n",
      "                    free = 2                spam : ham    =     91.7 : 1.0\n",
      "                     txt = 1                spam : ham    =     90.0 : 1.0\n",
      "                  urgent = 1                spam : ham    =     88.7 : 1.0\n",
      "                    code = 1                spam : ham    =     87.5 : 1.0\n",
      "                      16 = 1                spam : ham    =     87.5 : 1.0\n",
      "                      uk = 1                spam : ham    =     87.5 : 1.0\n",
      "                      po = 1                spam : ham    =     70.5 : 1.0\n",
      "                   nokia = 1                spam : ham    =     67.8 : 1.0\n",
      "                landline = 1                spam : ham    =     65.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier3 = NaiveBayesClassifier.train(train_set3)\n",
    "print(\"Accuracy :\", classify.accuracy(classifier3, test_set3))\n",
    "classifier3.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ทำ 10 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Featureset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9095152603231598\n",
      "0.9084380610412927\n",
      "0.9138240574506283\n",
      "0.9148499615285971\n",
      "0.90754039497307\n",
      "0.9095152603231598\n",
      "0.9151705565529623\n",
      "0.9072411729503291\n",
      "0.9129263913824057\n",
      "0.9102333931777379\n",
      "Average Accuracy :  0.9109254509703343\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "subset_size = len(featuresets1) // num_folds\n",
    "accu_list = []\n",
    "for i in range(num_folds):\n",
    "    testing_this_round = featuresets1[i*subset_size:]\n",
    "    training_this_round = featuresets1[:i*subset_size] + featuresets1[(i+1)*subset_size:]\n",
    "    \n",
    "    classifier = NaiveBayesClassifier.train(training_this_round)\n",
    "    accu = classify.accuracy(classifier, testing_this_round) \n",
    "    accu_list.append(accu)\n",
    "    print(accu)\n",
    "    \n",
    "avg_accu = sum(accu_list) / len(accu_list)\n",
    "print(\"Average Accuracy : \", avg_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Featureset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9481149012567325\n",
      "0.9463395172551367\n",
      "0.9481597845601436\n",
      "0.9443447037701975\n",
      "0.9437462597247157\n",
      "0.9396768402154398\n",
      "0.9344703770197487\n",
      "0.9317773788150808\n",
      "0.9192100538599641\n",
      "0.8940754039497307\n",
      "Average Accuracy :  0.934991522042689\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "subset_size = len(featuresets2) // num_folds\n",
    "accu_list = []\n",
    "for i in range(num_folds):\n",
    "    testing_this_round = featuresets2[i*subset_size:]\n",
    "    training_this_round = featuresets2[:i*subset_size] + featuresets2[(i+1)*subset_size:]\n",
    "    \n",
    "    classifier = NaiveBayesClassifier.train(training_this_round)\n",
    "    accu = classify.accuracy(classifier, testing_this_round) \n",
    "    accu_list.append(accu)\n",
    "    print(accu)\n",
    "    \n",
    "avg_accu = sum(accu_list) / len(accu_list)\n",
    "print(\"Average Accuracy : \", avg_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Featureset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9025134649910234\n",
      "0.9024536205864752\n",
      "0.9077648114901257\n",
      "0.908951013080277\n",
      "0.9006582884500299\n",
      "0.9041292639138241\n",
      "0.9084380610412927\n",
      "0.9036505086774387\n",
      "0.9021543985637342\n",
      "0.9120287253141831\n",
      "Average Accuracy :  0.9052742156108403\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "subset_size = len(featuresets3) // num_folds\n",
    "accu_list = []\n",
    "for i in range(num_folds):\n",
    "    testing_this_round = featuresets3[i*subset_size:]\n",
    "    training_this_round = featuresets3[:i*subset_size] + featuresets3[(i+1)*subset_size:]\n",
    "    \n",
    "    classifier = NaiveBayesClassifier.train(training_this_round)\n",
    "    accu = classify.accuracy(classifier, testing_this_round) \n",
    "    accu_list.append(accu)\n",
    "    print(accu)\n",
    "    \n",
    "avg_accu = sum(accu_list) / len(accu_list)\n",
    "print(\"Average Accuracy : \", avg_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## สรุปผลการทดลอง \n",
    "- จากการทำ cross validation จะเห็นได้ว่าวิธีการ Extract Feature ทั้งสามแบบให้ผลความแม่นยำที่ 90% ทั้งสามวิธี\n",
    "- อย่างไรก็ตาม วิธีที่ให้ผลลัพธ์ที่แม่นยำมากที่สุดคือวิธีการ Extract Feature แบบที่ 2 ซึ่งเป็นวิธีการที่อาศัยการคำนวนอัตราส่วนระหว่างตัวอักษรพิมพ์ใหญ่ต่อตัวอักษรทั้งหมด โดยให้ความแม่นยำโดยเฉลี่ยที่ 93%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## คำนวน Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = [sms[1] for sms in test_set1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [classifier.classify(sms[0]) for sms in test_set1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |       s |\n",
      "     |   h   p |\n",
      "     |   a   a |\n",
      "     |   m   m |\n",
      "-----+---------+\n",
      " ham |<862>  . |\n",
      "spam | 107<145>|\n",
      "-----+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "cm = ConfusionMatrix(ref, tagged)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham', 'spam'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {'ham', 'spam'}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 1007 Counter({'ham': 862, 'spam': 145})\n",
      "FN: 107 Counter({'spam': 107, 'ham': 0})\n",
      "FP: 107 Counter({'ham': 107, 'spam': 0})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n",
    "\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i,j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i,j]\n",
    "            false_positives[j] += cm[i,j]\n",
    "\n",
    "print(\"TP:\", sum(true_positives.values()), true_positives)\n",
    "print(\"FN:\", sum(false_negatives.values()), false_negatives)\n",
    "print(\"FP:\", sum(false_positives.values()), false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
