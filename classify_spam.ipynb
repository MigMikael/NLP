{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import ไลบรารี่ที่จำเป็น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, NaiveBayesClassifier\n",
    "from nltk import classify\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import os, glob, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words  ในภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlemmatizer = WordNetLemmatizer()\n",
    "commonwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ลองโหลด Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamtexts = []\n",
    "spamtexts = []\n",
    "\n",
    "for filename in glob.glob('/Users/migmikael/Downloads/enron1/ham/*.txt'):\n",
    "    fin = open(filename, \"r\",encoding='utf-8', errors='ignore')\n",
    "    hamtexts.append(fin.read())\n",
    "    fin.close()\n",
    "    \n",
    "for filename in glob.glob('/Users/migmikael/Downloads/enron1/spam/*.txt'):\n",
    "    fin = open(filename, \"r\",encoding='utf-8', errors='ignore')\n",
    "    spamtexts.append(fin.read())\n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Subject: what up , , your cam babe\\nwhat are you looking for ?\\nif your looking for a companion for friendship , love , a date , or just good ole '\\nfashioned * * * * * * , then try our brand new site ; it was developed and created\\nto help anyone find what they ' re looking for . a quick bio form and you ' re\\non the road to satisfaction in every sense of the word . . . . no matter what\\nthat may be !\\ntry it out and youll be amazed .\\nhave a terrific time this evening\\ncopy and pa ste the add . ress you see on the line below into your browser to come to the site .\\nhttp : / / www . meganbang . biz / bld / acc /\\nno more plz\\nhttp : / / www . naturalgolden . com / retract /\\ncounterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\\n\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spamtexts[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# นำข้อมูลทั้งสองชนิดมาผสมกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixemail = [(email, 'spam') for email in spamtexts]\n",
    "mixemail += [(email, 'ham') for email in hamtexts]\n",
    "\n",
    "random.shuffle(mixemail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# แกะ Feature \n",
    "- แบ่งประโยคที่รับมาด้วย Tokenizer แปลงเป็นพิมพ์เล็กด้วย .lower และหารากศัพย์ด้วย wordlemmatizer\n",
    "- สร้าง feature dictionary ด้วย wordtokens ที่ได้จากขั้นตอนก่อนหน้า โดยตัดเอา stop word ออก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(sent):\n",
    "    features = {}\n",
    "    wordtokens = [wordlemmatizer.lemmatize(word.lower()) for word in word_tokenize(sent)]\n",
    "    \n",
    "    for word in wordtokens:\n",
    "        if word not in commonwords:\n",
    "            features[word] = True\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': True, 'iron': True, 'man': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(\"I am the iron man.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# สร้างข้อมูลจาก Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(feature_extractor(email), label) for (email, label) in mixemail]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({',': True,\n",
       "   '.': True,\n",
       "   '/': True,\n",
       "   '000': True,\n",
       "   '120': True,\n",
       "   '20': True,\n",
       "   '2000': True,\n",
       "   '28': True,\n",
       "   ':': True,\n",
       "   ';': True,\n",
       "   'actuals': True,\n",
       "   'august': True,\n",
       "   'daily': True,\n",
       "   'enron': True,\n",
       "   'gas': True,\n",
       "   'hpl': True,\n",
       "   'ic': True,\n",
       "   'l': True,\n",
       "   'lsk': True,\n",
       "   'subject': True,\n",
       "   'tap': True,\n",
       "   'teco': True},\n",
       "  'ham')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# แบ่งข้อมูลออกเป็น Train / Test ที่ 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set size = 4137, test_set size = 1035\n"
     ]
    }
   ],
   "source": [
    "size = int(len(featuresets) * 0.8)\n",
    "train_set, test_set = featuresets[:size], featuresets[size:]\n",
    "print(\"train_set size = %d, test_set size = %d\" % (len(train_set), len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# เทรนตัว classifier ด้วย Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458937198067633\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "print(classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            prescription = True             spam : ham    =    155.0 : 1.0\n",
      "                    2004 = True             spam : ham    =    145.1 : 1.0\n",
      "                     nom = True              ham : spam   =    119.4 : 1.0\n",
      "                   meter = True              ham : spam   =    106.9 : 1.0\n",
      "                    pain = True             spam : ham    =    103.6 : 1.0\n",
      "              nomination = True              ham : spam   =     94.8 : 1.0\n",
      "                   cheap = True             spam : ham    =     93.7 : 1.0\n",
      "                    spam = True             spam : ham    =     90.4 : 1.0\n",
      "                  dealer = True             spam : ham    =     87.0 : 1.0\n",
      "                     sex = True             spam : ham    =     87.0 : 1.0\n",
      "                     ect = True              ham : spam   =     83.0 : 1.0\n",
      "                featured = True             spam : ham    =     80.4 : 1.0\n",
      "                  differ = True             spam : ham    =     80.4 : 1.0\n",
      "                  reader = True             spam : ham    =     77.1 : 1.0\n",
      "                    2001 = True              ham : spam   =     73.2 : 1.0\n",
      "                creative = True             spam : ham    =     68.8 : 1.0\n",
      "                      cc = True              ham : spam   =     64.8 : 1.0\n",
      "                     713 = True              ham : spam   =     63.1 : 1.0\n",
      "                   epson = True             spam : ham    =     58.9 : 1.0\n",
      "                inherent = True             spam : ham    =     57.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ลองทำ 10 Fold Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458413926499033\n",
      "0.9497098646034816\n",
      "0.9439071566731141\n",
      "0.9439071566731141\n",
      "0.9497098646034816\n",
      "0.9516441005802708\n",
      "0.9477756286266924\n",
      "0.9439071566731141\n",
      "0.941972920696325\n",
      "0.9439071566731141\n",
      "Average Accuracy :  0.9462282398452612\n"
     ]
    }
   ],
   "source": [
    "num_folds = 10\n",
    "subset_size = len(featuresets) // num_folds\n",
    "accu_list = []\n",
    "for i in range(num_folds):\n",
    "    testing_this_round = featuresets[i*subset_size:][:subset_size]\n",
    "    training_this_round = featuresets[:i*subset_size] + featuresets[(i+1)*subset_size:]\n",
    "    \n",
    "    classifier = NaiveBayesClassifier.train(training_this_round)\n",
    "    accu = classify.accuracy(classifier, testing_this_round) \n",
    "    accu_list.append(accu)\n",
    "    print(accu)\n",
    "    \n",
    "    #print(len(testing_this_round))\n",
    "    #print(len(training_this_round))\n",
    "    #print()\n",
    "avg_accu = sum(accu_list) / len(accu_list)\n",
    "print(\"Average Accuracy : \", avg_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
